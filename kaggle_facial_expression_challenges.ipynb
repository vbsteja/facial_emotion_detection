{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_facial_expression_challenges.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbsteja/facial_emotion_detection/blob/master/kaggle_facial_expression_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_9Lxoky_qdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI5-UMMurCzz",
        "colab_type": "text"
      },
      "source": [
        "### PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFir1eGm5F-r",
        "colab_type": "code",
        "outputId": "01c14d1a-5694-400d-bfcc-beb5e6f0252d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!wget https://cdn.pixabay.com/photo/2014/11/30/14/11/kitty-551554__340.jpg\n",
        "!pip install tensorflow-gpu==1.15\n",
        "!pip install --user kaggle\n",
        "!rm -r /root/.kaggle\n",
        "!mkdir /root/.kaggle\n",
        "!echo '{\"username\":\"vbsuryateja2\",\"key\":\"***************\"}' > /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "#!unzip test1.zip\n",
        "#!unzip train.zip\n",
        "!mkdir weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.1.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 55.4MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15) (41.6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow-gpu 1.14.0\n",
            "    Uninstalling tensorflow-gpu-1.14.0:\n",
            "      Successfully uninstalled tensorflow-gpu-1.14.0\n",
            "Successfully installed tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "example_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "fer2013.tar.gz: Skipping, found more recently modified local copy (use --force to force download)\n",
            "mkdir: cannot create directory ‘weights’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFPo8RXxCAH",
        "colab_type": "code",
        "outputId": "f65f4f1e-16f5-4bab-d8a0-936dd15050fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!tar -xvf  fer2013.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fer2013/fer2013.csv\n",
            "fer2013/README\n",
            "fer2013/fer2013.bib\n",
            "fer2013/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWu66wUEBKqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.advanced_activations import ELU,Softmax\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "#from config import emotion_config as config\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "import argparse\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_12MTt7QrHRn",
        "colab_type": "text"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOFt8nzJqDwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmotionVGGNet:\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height,width,depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\tif K.image_data_format == \"channels_first\":\n",
        "\t\t\tinputShape = (depth,height,width)\n",
        "\t\t\tchanDim = 1\n",
        "\t\tmodel = Sequential([\n",
        "\t\t\tConv2D(32,(3,3), padding=\"same\",\n",
        "\t\t\tkernel_initializer=\"he_normal\",input_shape=inputShape),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(axis=chanDim),\n",
        "\t\t\tConv2D(32,(3,3), kernel_initializer=\"he_normal\",\n",
        "\t\t\t\tpadding=\"same\"),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(axis=chanDim),\n",
        "\t\t\tMaxPooling2D(pool_size=(2,2)),\n",
        "\t\t\tDropout(0,25),\n",
        "\t\t\tConv2D(64,(3,3), padding=\"same\",\n",
        "\t\t\tkernel_initializer=\"he_normal\",input_shape=inputShape),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(axis=chanDim),\n",
        "\t\t\tConv2D(64,(3,3), kernel_initializer=\"he_normal\",\n",
        "\t\t\t\tpadding=\"same\"),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(axis=chanDim),\n",
        "\t\t\tMaxPooling2D(pool_size=(2,2)),\n",
        "\t\t\tDropout(0,25),\n",
        "\t\t\tConv2D(64,(3,3), padding=\"same\",\n",
        "\t\t\tkernel_initializer=\"he_normal\",input_shape=inputShape),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(axis=chanDim),\n",
        "\t\t\tConv2D(64,(3,3), kernel_initializer=\"he_normal\",\n",
        "\t\t\t\tpadding=\"same\"),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(axis=chanDim),\n",
        "\t\t\tMaxPooling2D(pool_size=(2,2)),\n",
        "\t\t\tDropout(0,25),\n",
        "\t\t\tFlatten(),\n",
        "\t\t\tDense(64,kernel_initializer=\"he_normal\"),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(),\n",
        "\t\t\tDropout(0.25),\n",
        "\t\t\tDense(64,kernel_initializer=\"he_normal\"),\n",
        "\t\t\tELU(),\n",
        "\t\t\tBatchNormalization(),\n",
        "\t\t\tDropout(0.25),\n",
        "\t\t\tDense(classes,kernel_initializer=\"he_normal\"),\n",
        "\t\t\tSoftmax()])\n",
        "\t\treturn model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlyyI36brOFh",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeLLlLjQxqsP",
        "colab_type": "code",
        "outputId": "600fa042-1b86-4d09-978f-b35a22ac2f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#CONST\n",
        "emotion_dict = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
        "BASE_DIR = \"fer2013/\"\n",
        "data = pd.read_csv(BASE_DIR+\"fer2013.csv\")\n",
        "data.loc[data[\"emotion\"]==1,[\"emotion\"]] = 0\n",
        "data.head()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tpTysRHzRq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pixels = [np.array([int(i) for i in j.split()]) for j in data[data.Usage == \"Training\"][\"pixels\"].values]\n",
        "train_pixels = np.array([i.reshape((48,48)) for i in train_pixels])\n",
        "train_pixels = np.expand_dims(train_pixels,-1)\n",
        "train_labels = np.array([int(j) for j in data[data.Usage == 'Training'][\"emotion\"].values])\n",
        "train_labels = np.array(to_categorical(train_labels,num_classes = 7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ricncww774if",
        "colab_type": "code",
        "outputId": "7d648085-e39e-460f-bc3b-a00a16503fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.emotion.unique()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 4, 6, 3, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go3JsPf-6mo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pixels = [np.array([int(i) for i in j.split()]) for j in data[data.Usage == \"PrivateTest\"][\"pixels\"].values]\n",
        "test_pixels = np.array([i.reshape((48,48)) for i in test_pixels])\n",
        "test_pixels = np.expand_dims(test_pixels,-1)\n",
        "test_labels = np.array([int(j) for j in data[data.Usage == 'PrivateTest'][\"emotion\"].values])\n",
        "test_labels = np.array(to_categorical(test_labels,num_classes = 7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXH2MN2s7j5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_pixels = [np.array([int(i) for i in j.split()]) for j in data[data.Usage == \"PublicTest\"][\"pixels\"].values]\n",
        "validation_pixels = np.array([i.reshape((48,48)) for i in validation_pixels])\n",
        "validation_pixels = np.expand_dims(validation_pixels,-1)\n",
        "validation_labels = np.array([int(j) for j in data[data.Usage == 'PublicTest'][\"emotion\"].values])\n",
        "validation_labels = np.array(to_categorical(validation_labels,num_classes = 7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sW69prk1cDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VId4Ev62qVe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = ImageDataGenerator(rotation_range=10, zoom_range=0.1,horizontal_flip=True, rescale=1 / 255.0, fill_mode=\"nearest\")\n",
        "test = ImageDataGenerator(rescale=1 / 255.0)\n",
        "validation = ImageDataGenerator(rescale=1 / 255.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk4lOZHQqgYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9MYieb6rcwW",
        "colab_type": "text"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTa_76W3zlhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"[INFO] compiling model...\")\n",
        "model = EmotionVGGNet.build(width=48, height=48, depth=1,classes=len(emotion_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmeVKEj4qiaH",
        "colab_type": "code",
        "outputId": "f74acc79-2dfd-4fde-bbfc-440e1f3fac74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "opt = Adam(lr=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='weights/EmotionVGGNet.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit_generator(\n",
        "\ttrain.flow(train_pixels,train_labels,batch_size=32),\n",
        "\tvalidation_data = validation.flow(validation_pixels,validation_labels),\n",
        "\tepochs = 30,\n",
        "\tcallbacks = [checkpointer],\n",
        "\tverbose=1)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "898/898 [==============================] - 20s 22ms/step - loss: 1.0498 - acc: 0.6044 - val_loss: 1.0465 - val_acc: 0.6135\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.04645, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 2/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 1.0185 - acc: 0.6187 - val_loss: 1.0574 - val_acc: 0.6133\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.04645\n",
            "Epoch 3/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 1.0146 - acc: 0.6214 - val_loss: 1.0313 - val_acc: 0.6213\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.04645 to 1.03134, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 4/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 1.0042 - acc: 0.6257 - val_loss: 1.0386 - val_acc: 0.6197\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.03134\n",
            "Epoch 5/30\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 0.9970 - acc: 0.6262 - val_loss: 1.0146 - val_acc: 0.6236\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.03134 to 1.01463, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 6/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9900 - acc: 0.6311 - val_loss: 1.0067 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.01463 to 1.00668, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 7/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9878 - acc: 0.6296 - val_loss: 1.0619 - val_acc: 0.6158\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.00668\n",
            "Epoch 8/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9841 - acc: 0.6320 - val_loss: 1.0310 - val_acc: 0.6188\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.00668\n",
            "Epoch 9/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9825 - acc: 0.6316 - val_loss: 1.0266 - val_acc: 0.6211\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.00668\n",
            "Epoch 10/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9810 - acc: 0.6345 - val_loss: 1.0086 - val_acc: 0.6264\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.00668\n",
            "Epoch 11/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9733 - acc: 0.6373 - val_loss: 1.0299 - val_acc: 0.6135\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.00668\n",
            "Epoch 12/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9683 - acc: 0.6430 - val_loss: 1.0354 - val_acc: 0.6213\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.00668\n",
            "Epoch 13/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9744 - acc: 0.6385 - val_loss: 1.0165 - val_acc: 0.6269\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.00668\n",
            "Epoch 14/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9638 - acc: 0.6429 - val_loss: 1.0155 - val_acc: 0.6241\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.00668\n",
            "Epoch 15/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9617 - acc: 0.6442 - val_loss: 1.0046 - val_acc: 0.6258\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.00668 to 1.00455, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 16/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9580 - acc: 0.6440 - val_loss: 1.0205 - val_acc: 0.6239\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.00455\n",
            "Epoch 17/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9642 - acc: 0.6440 - val_loss: 0.9919 - val_acc: 0.6314\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.00455 to 0.99190, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 18/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9531 - acc: 0.6471 - val_loss: 0.9974 - val_acc: 0.6258\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.99190\n",
            "Epoch 19/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9557 - acc: 0.6480 - val_loss: 1.0002 - val_acc: 0.6297\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.99190\n",
            "Epoch 20/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9483 - acc: 0.6480 - val_loss: 0.9869 - val_acc: 0.6258\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.99190 to 0.98692, saving model to weights/EmotionVGGNet.hdf5\n",
            "Epoch 21/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9470 - acc: 0.6524 - val_loss: 1.0066 - val_acc: 0.6255\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.98692\n",
            "Epoch 22/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9370 - acc: 0.6547 - val_loss: 1.0040 - val_acc: 0.6252\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.98692\n",
            "Epoch 23/30\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 0.9423 - acc: 0.6527 - val_loss: 1.0056 - val_acc: 0.6278\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.98692\n",
            "Epoch 24/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9386 - acc: 0.6559 - val_loss: 1.0081 - val_acc: 0.6266\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.98692\n",
            "Epoch 25/30\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 0.9328 - acc: 0.6550 - val_loss: 1.0293 - val_acc: 0.6230\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.98692\n",
            "Epoch 26/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9397 - acc: 0.6502 - val_loss: 0.9969 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.98692\n",
            "Epoch 27/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9315 - acc: 0.6577 - val_loss: 1.0060 - val_acc: 0.6319\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.98692\n",
            "Epoch 28/30\n",
            "898/898 [==============================] - 17s 19ms/step - loss: 0.9280 - acc: 0.6573 - val_loss: 1.0083 - val_acc: 0.6216\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.98692\n",
            "Epoch 29/30\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 0.9194 - acc: 0.6600 - val_loss: 1.0086 - val_acc: 0.6266\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.98692\n",
            "Epoch 30/30\n",
            "898/898 [==============================] - 17s 18ms/step - loss: 0.9209 - acc: 0.6602 - val_loss: 0.9928 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.98692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8e951da20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcYEWDeprsBN",
        "colab_type": "text"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z72JKtvRq0V3",
        "colab_type": "code",
        "outputId": "290bbb4c-ed54-4b62-b816-23ff61377db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#model = load_model(\"weights/EmotionVGGNet.hdf5\")\n",
        "\n",
        "loss, acc = model.evaluate_generator(\n",
        "test.flow(test_pixels,test_labels))\n",
        "\n",
        "print(\"[INFO] accuracy : {:.2f}\".format(acc*100))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] accuracy : 67.85\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
